
@article{pevny_using_2016,
	title = {Using Neural Network Formalism to Solve Multiple-Instance Problems},
	url = {http://arxiv.org/abs/1609.07257},
	abstract = {Many objects in the real world are difficult to describe by a single numerical vector of a fixed length, whereas describing them by a set of vectors is more natural. Therefore, Multiple instance learning ({MIL}) techniques have been constantly gaining on importance throughout last years. {MIL} formalism represents each object (sample) by a set (bag) of feature vectors (instances) of fixed length where knowledge about objects (e.g., class label) is available on bag level but not necessarily on instance level. Many standard tools including supervised classifiers have been already adapted to {MIL} setting since the problem got formalized in late nineties. In this work we propose a neural network ({NN}) based formalism that intuitively bridges the gap between {MIL} problem definition and the vast existing knowledge-base of standard models and classifiers. We show that the proposed {NN} formalism is effectively optimizable by a modified back-propagation algorithm and can reveal unknown patterns inside bags. Comparison to eight types of classifiers from the prior art on a set of 14 publicly available benchmark datasets confirms the advantages and accuracy of the proposed solution.},
	journaltitle = {{arXiv}:1609.07257 [cs, stat]},
	author = {Pevny, Tomas and Somol, Petr},
	urldate = {2017-03-01},
	date = {2016-09-23},
	eprinttype = {arxiv},
	eprint = {1609.07257},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/cisco/.mozilla/firefox/wqy8gqj5.default/zotero/storage/TMZARQ23/1609.html:text/html}
}

@inproceedings{pevny_discriminative_2016,
	location = {New York, {NY}, {USA}},
	title = {Discriminative Models for Multi-instance Problems with Tree Structure},
	isbn = {978-1-4503-4573-6},
	url = {http://doi.acm.org/10.1145/2996758.2996761},
	doi = {10.1145/2996758.2996761},
	series = {{AISec} '16},
	abstract = {Modelling network traffic is gaining importance to counter modern security threats of ever increasing sophistication. It is though surprisingly difficult and costly to construct reliable classifiers on top of telemetry data due to the variety and complexity of signals that no human can manage to interpret in full. Obtaining training data with sufficiently large and variable body of labels can thus be seen as a prohibitive problem. The goal of this work is to detect infected computers by observing their {HTTP}(S) traffic collected from network sensors, which are typically proxy servers or network firewalls, while relying on only minimal human input in the model training phase. We propose a discriminative model that makes decisions based on a computer's all traffic observed during a predefined time window (5 minutes in our case). The model is trained on traffic samples collected over equally-sized time windows for a large number of computers, where the only labels needed are (human) verdicts about the computer as a whole (presumed infected vs. presumed clean). As part of training, the model itself learns discriminative patterns in traffic targeted to individual servers and constructs the final high-level classifier on top of them. We show the classifier to perform with very high precision, and demonstrate that the learned traffic patterns can be interpreted as Indicators of Compromise. We implement the discriminative model as a neural network with special structure reflecting two stacked multi instance problems. The main advantages of the proposed configuration include not only improved accuracy and ability to learn from gross labels, but also automatic learning of server types (together with their detectors) that are typically visited by infected computers.},
	pages = {83--91},
	booktitle = {Proceedings of the 2016 {ACM} Workshop on Artificial Intelligence and Security},
	publisher = {{ACM}},
	author = {Pevny, Tomas and Somol, Petr},
	urldate = {2017-03-01},
	date = {2016},
	keywords = {big data, learning indicators of compromise, malware detection, neural network, user modeling}
}

@article{gulcehre_noisy_2016,
	title = {Noisy Activation Functions},
	url = {http://arxiv.org/abs/1603.00391},
	abstract = {Common nonlinear activation functions used in neural networks can cause training difficulties due to the saturation behavior of the activation function, which may hide dependencies that are not visible to vanilla-{SGD} (using first order gradients only). Gating mechanisms that use softly saturating activation functions to emulate the discrete switching of digital logic circuits are good examples of this. We propose to exploit the injection of appropriate noise so that the gradients may flow easily, even if the noiseless application of the activation function would yield zero gradient. Large noise will dominate the noise-free gradient and allow stochastic gradient descent toexplore more. By adding noise only to the problematic parts of the activation function, we allow the optimization procedure to explore the boundary between the degenerate (saturating) and the well-behaved parts of the activation function. We also establish connections to simulated annealing, when the amount of noise is annealed down, making it easier to optimize hard objective functions. We find experimentally that replacing such saturating activation functions by noisy variants helps training in many contexts, yielding state-of-the-art or competitive results on different datasets and task, especially when training seems to be the most difficult, e.g., when curriculum learning is necessary to obtain good results.},
	journaltitle = {{arXiv}:1603.00391 [cs, stat]},
	author = {Gulcehre, Caglar and Moczulski, Marcin and Denil, Misha and Bengio, Yoshua},
	urldate = {2017-03-06},
	date = {2016-03-01},
	eprinttype = {arxiv},
	eprint = {1603.00391},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv\:1603.00391 PDF:/home/cisco/.mozilla/firefox/wqy8gqj5.default/zotero/storage/6XC4J6PI/Gulcehre et al. - 2016 - Noisy Activation Functions.pdf:application/pdf;arXiv.org Snapshot:/home/cisco/.mozilla/firefox/wqy8gqj5.default/zotero/storage/ABWZSR2D/1603.html:text/html}
}

@article{czarnecki_understanding_2017,
	title = {Understanding Synthetic Gradients and Decoupled Neural Interfaces},
	url = {http://arxiv.org/abs/1703.00522},
	abstract = {When training neural networks, the use of Synthetic Gradients ({SG}) allows layers or modules to be trained without update locking - without waiting for a true error gradient to be backpropagated - resulting in Decoupled Neural Interfaces ({DNIs}). This unlocked ability of being able to update parts of a neural network asynchronously and with only local information was demonstrated to work empirically in Jaderberg et al (2016). However, there has been very little demonstration of what changes {DNIs} and {SGs} impose from a functional, representational, and learning dynamics point of view. In this paper, we study {DNIs} through the use of synthetic gradients on feed-forward networks to better understand their behaviour and elucidate their effect on optimisation. We show that the incorporation of {SGs} does not affect the representational strength of the learning system for a neural network, and prove the convergence of the learning system for linear and deep linear models. On practical problems we investigate the mechanism by which synthetic gradient estimators approximate the true loss, and, surprisingly, how that leads to drastically different layer-wise representations. Finally, we also expose the relationship of using synthetic gradients to other error approximation techniques and find a unifying language for discussion and comparison.},
	journaltitle = {{arXiv}:1703.00522 [cs]},
	author = {Czarnecki, Wojciech Marian and Świrszcz, Grzegorz and Jaderberg, Max and Osindero, Simon and Vinyals, Oriol and Kavukcuoglu, Koray},
	urldate = {2017-03-27},
	date = {2017-03-01},
	eprinttype = {arxiv},
	eprint = {1703.00522},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1703.00522 PDF:/home/cisco/.mozilla/firefox/wqy8gqj5.default/zotero/storage/KAFXM9H3/Czarnecki et al. - 2017 - Understanding Synthetic Gradients and Decoupled Ne.pdf:application/pdf;arXiv.org Snapshot:/home/cisco/.mozilla/firefox/wqy8gqj5.default/zotero/storage/4KE332W9/1703.html:text/html}
}

@article{vinyals_order_2015,
	title = {Order Matters: Sequence to sequence for sets},
	url = {http://arxiv.org/abs/1511.06391},
	shorttitle = {Order Matters},
	abstract = {Sequences have become first class citizens in supervised learning thanks to the resurgence of recurrent neural networks. Many complex tasks that require mapping from or to a sequence of observations can now be formulated with the sequence-to-sequence (seq2seq) framework which employs the chain rule to efficiently represent the joint probability of sequences. In many cases, however, variable sized inputs and/or outputs might not be naturally expressed as sequences. For instance, it is not clear how to input a set of numbers into a model where the task is to sort them; similarly, we do not know how to organize outputs when they correspond to random variables and the task is to model their unknown joint probability. In this paper, we first show using various examples that the order in which we organize input and/or output data matters significantly when learning an underlying model. We then discuss an extension of the seq2seq framework that goes beyond sequences and handles input sets in a principled way. In addition, we propose a loss which, by searching over possible orders during training, deals with the lack of structure of output sets. We show empirical evidence of our claims regarding ordering, and on the modifications to the seq2seq framework on benchmark language modeling and parsing tasks, as well as two artificial tasks -- sorting numbers and estimating the joint probability of unknown graphical models.},
	journaltitle = {{arXiv}:1511.06391 [cs, stat]},
	author = {Vinyals, Oriol and Bengio, Samy and Kudlur, Manjunath},
	urldate = {2017-04-19},
	date = {2015-11-19},
	eprinttype = {arxiv},
	eprint = {1511.06391},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv\:1511.06391 PDF:/home/cisco/.mozilla/firefox/wqy8gqj5.default/zotero/storage/ASJ97XD2/Vinyals et al. - 2015 - Order Matters Sequence to sequence for sets.pdf:application/pdf;arXiv.org Snapshot:/home/cisco/.mozilla/firefox/wqy8gqj5.default/zotero/storage/WSUZ6KAK/1511.html:text/html}
}

@article{kingma_adam:_2014,
	title = {Adam: A Method for Stochastic Optimization},
	url = {http://arxiv.org/abs/1412.6980},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss {AdaMax}, a variant of Adam based on the infinity norm.},
	journaltitle = {{arXiv}:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	urldate = {2017-04-19},
	date = {2014-12-22},
	eprinttype = {arxiv},
	eprint = {1412.6980},
	keywords = {Computer Science - Learning},
	file = {arXiv\:1412.6980 PDF:/home/cisco/.mozilla/firefox/wqy8gqj5.default/zotero/storage/F3NRV8A4/Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/home/cisco/.mozilla/firefox/wqy8gqj5.default/zotero/storage/UHTTRZWN/1412.html:text/html}
}

@online{mazur_step_2015,
	title = {A Step by Step Backpropagation Example},
	url = {https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/},
	abstract = {Background Backpropagation is a common method for training a neural network. There is no shortage of papers online that attempt to explain how backpropagation works, but few that include an example…},
	titleaddon = {Matt Mazur},
	author = {{Mazur}},
	urldate = {2017-04-19},
	date = {2015-03-17},
	file = {Snapshot:/home/cisco/.mozilla/firefox/wqy8gqj5.default/zotero/storage/DZNJXE2T/a-step-by-step-backpropagation-example.html:text/html}
}

@article{edwards_towards_2016,
	title = {Towards a Neural Statistician},
	url = {http://arxiv.org/abs/1606.02185},
	abstract = {An efficient learner is one who reuses what they already know to tackle a new problem. For a machine learner, this means understanding the similarities amongst datasets. In order to do this, one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model. Towards this goal, we demonstrate an extension of a variational autoencoder that can learn a method for computing representations, or statistics, of datasets in an unsupervised fashion. The network is trained to produce statistics that encapsulate a generative model for each dataset. Hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks. We show that we are able to learn statistics that can be used for: clustering datasets, transferring generative models to new datasets, selecting representative samples of datasets and classifying previously unseen classes. We refer to our model as a neural statistician, and by this we mean a neural network that can learn to compute summary statistics of datasets without supervision.},
	journaltitle = {{arXiv}:1606.02185 [cs, stat]},
	author = {Edwards, Harrison and Storkey, Amos},
	urldate = {2017-05-28},
	date = {2016-06-07},
	eprinttype = {arxiv},
	eprint = {1606.02185},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv\:1606.02185 PDF:/home/cisco/.mozilla/firefox/wqy8gqj5.default/zotero/storage/JD4UT5UD/Edwards and Storkey - 2016 - Towards a Neural Statistician.pdf:application/pdf;arXiv.org Snapshot:/home/cisco/.mozilla/firefox/wqy8gqj5.default/zotero/storage/B9BNT4Z8/1606.html:text/html}
}

@inproceedings{bartos_optimized_2016,
	title = {Optimized invariant representation of network traffic for detecting unseen malware variants},
	url = {https://www.usenix.org/system/files/conference/usenixsecurity16/sec16_paper_bartos.pdf},
	booktitle = {J25th {USENIX} Security Symposium ({USENIX} Security 16). {USENIX} Association},
	author = {Bartos, Karel and Sofka, Michal and Franc, Vojtech},
	urldate = {2017-05-28},
	date = {2016},
	file = {[PDF] usenix.org:/home/cisco/.mozilla/firefox/wqy8gqj5.default/zotero/storage/3VETSZI8/Bartos et al. - 2016 - Optimized invariant representation of network traf.pdf:application/pdf}
}

@article{anderson_deciphering_2016,
	title = {Deciphering Malware's use of {TLS} (without Decryption)},
	url = {http://arxiv.org/abs/1607.01639},
	abstract = {The use of {TLS} by malware poses new challenges to network threat detection because traditional pattern-matching techniques can no longer be applied to its messages. However, {TLS} also introduces a complex set of observable data features that allow many inferences to be made about both the client and the server. We show that these features can be used to detect and understand malware communication, while at the same time preserving the privacy of benign uses of encryption. These data features also allow for accurate malware family attribution of network communication, even when restricted to a single, encrypted flow. To demonstrate this, we performed a detailed study of how {TLS} is used by malware and enterprise applications. We provide a general analysis on millions of {TLS} encrypted flows, and a targeted study on 18 malware families composed of thousands of unique malware samples and ten-of-thousands of malicious {TLS} flows. Importantly, we identify and accommodate the bias introduced by the use of a malware sandbox. The performance of a malware classifier is correlated with a malware family's use of {TLS}, i.e., malware families that actively evolve their use of cryptography are more difficult to classify. We conclude that malware's usage of {TLS} is distinct from benign usage in an enterprise setting, and that these differences can be effectively used in rules and machine learning classifiers.},
	journaltitle = {{arXiv}:1607.01639 [cs]},
	author = {Anderson, Blake and Paul, Subharthi and {McGrew}, David},
	urldate = {2017-05-28},
	date = {2016-07-06},
	eprinttype = {arxiv},
	eprint = {1607.01639},
	keywords = {Computer Science - Cryptography and Security},
	file = {arXiv\:1607.01639 PDF:/home/cisco/.mozilla/firefox/wqy8gqj5.default/zotero/storage/XPUFNA7I/Anderson et al. - 2016 - Deciphering Malware's use of TLS (without Decrypti.pdf:application/pdf;arXiv.org Snapshot:/home/cisco/.mozilla/firefox/wqy8gqj5.default/zotero/storage/Q6K7FQHI/1607.html:text/html}
}